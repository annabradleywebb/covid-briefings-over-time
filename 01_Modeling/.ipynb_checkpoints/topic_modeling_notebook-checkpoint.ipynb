{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "us_df = pickle.load(open(\"us_df.p\", \"rb\"))\n",
    "it_df = pickle.load(open(\"it_df.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set(nltk.corpus.stopwords.words('english'))\n",
    "english_words = english_words.union(['mr', 'president'])\n",
    "italian_words = set(nltk.corpus.stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV with single words, English\n",
    "cv_1_en = CountVectorizer(stop_words=english_words, ngram_range=(1,1))\n",
    "X_en_1 = cv_1_en.fit_transform(us_df['Content'])\n",
    "word_counts_en_1 = pd.DataFrame(X_en_1.toarray(),columns=cv_1_en.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_1_it = CountVectorizer(stop_words=italian_words, ngram_range=(1,1))\n",
    "X_it_1 = cv_1_it.fit_transform(it_df['Content'])\n",
    "word_counts_it_1 = pd.DataFrame(X_it_1.toarray(),columns=cv_1_it.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_2_en = CountVectorizer(stop_words=english_words, ngram_range=(2,2))\n",
    "X_en_2 = cv_2_en.fit_transform(us_df['Content'])\n",
    "word_counts_en_2 = pd.DataFrame(X_en_2.toarray(),columns=cv_2_en.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_2_it = CountVectorizer(stop_words=italian_words, ngram_range=(2,2))\n",
    "X_it_2 = cv_2_it.fit_transform(it_df['Content'])\n",
    "word_counts_it_2 = pd.DataFrame(X_it_2.toarray(),columns=cv_2_it.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58197197, 0.03302128, 0.02979686, 0.02717095, 0.02489888])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(en_1)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    \"\"\"\n",
    "    Displays the top n terms in each topic\n",
    "    \"\"\"\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix + 1)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "almost deal, antibody test, amazing great, atkinson job, back announcement\n",
      "\n",
      "Topic  2\n",
      "asymptomatic spreading, away something, asymptomatic monitoring, arms yes, back move\n",
      "\n",
      "Topic  3\n",
      "asymptomatic spreading, alone eighteen, back gave, approved within, asked want\n",
      "\n",
      "Topic  4\n",
      "asymptomatic monitoring, almost deal, asking administration, almost miracle, asymptomatic piece\n",
      "\n",
      "Topic  5\n",
      "almost million, asking administration, away never, amounts blue, back quickly\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, cv_2_en.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(2)\n",
    "doc_topic = nmf_model.fit_transform(en_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "going, know, people, think, want, said, like, lot, get, really, one, go, new, would, well, great, states, right, country, say\n",
      "\n",
      "Topic  2\n",
      "thank, people, going, want, get, us, testing, much, american, vice, working, know, states, think, also, need, well, governor, country, today\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, cv_1_en.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
